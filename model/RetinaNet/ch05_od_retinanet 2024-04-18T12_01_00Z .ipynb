{"cells":[{"cell_type":"markdown","metadata":{"id":"sguzjTkGKMXD"},"source":["# 物体検出　RetinaNet\n","+ 1段階検出器の有名なNetwork\n","+ CNNを使用する"]},{"cell_type":"markdown","metadata":{"id":"eJvAGv67Kd0_"},"source":["## インポート"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufB8TQP-LKaW"},"outputs":[],"source":["# Standard\n","import os\n","import sys\n","import glob\n","from collections import deque\n","import random\n","import copy\n","from copy import deepcopy\n","from typing import Callable, Sequence, Tuple, Union\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CskXUZIZMHVR"},"outputs":[],"source":["# 3rd-party\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pT9JNlgrKhTZ"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch import optim\n","\n","# torchvision\n","import torchvision\n","from torchvision import transforms as T\n","from torchvision.transforms import functional as TF\n","from torchvision.utils import draw_bounding_boxes\n","from torchvision.ops import sigmoid_focal_loss, batched_nms\n","from torchvision.ops.misc import FrozenBatchNorm2d\n","\n","# Coco\n","from pycocotools.cocoeval import COCOeval\n"]},{"cell_type":"markdown","metadata":{"id":"fzv6Uk2IM5hO"},"source":["## Google Drive マウント"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MzgiuSMB7oKe"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"JTmzqefqOG-k"},"source":["## IoUの計算方法\n","+ Intersection over Union"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3SOKrEtM9se"},"outputs":[],"source":["'''\n","boxes1: 矩形集合, [矩形数, 4 (min_x, min_y, max_x, max_y)]\n","boxes2: 矩形集合, [矩形数, 4 (min_x, min_y, max_x, max_y)]\n","'''\n","def calc_iou(boxes1: torch.Tensor,\n","             boxes2: torch.Tensor,\n","             ):\n","    # boxes1の第一軸をunsqueezeしてブロードキャスト計算可能にする\n","    # [矩形数, 2 (min_x, min_y) or (max_x, max_y)] -\u003e [矩形数, 1, 2]\n","    # [矩形数, 1, 2]と[矩形数, 2]の演算が[矩形数, 矩形数, 2]になる\n","\n","    # 積集合の左上の座標を取得\n","    intersection_left_top = torch.maximum(\n","        boxes1[:, :2].unsqueeze(dim=1), # [矩形数, 1, 2(min_x, min_y)]\n","        boxes2[:, :2], # [矩形数, 2(min_x, min_y)]\n","    )\n","\n","    # 積集合の右下の座標を取得\n","    intersection_right_down = torch.minimum(\n","        boxes1[:, 2:].unsqueeze(dim=1), # [矩形数, 1, 2(max_x, max_y)])\n","        boxes2[:, 2:], # [矩形数, 2(max_x, max_y)]\n","    )\n","\n","    # 重なる領域の面積を計算\n","    # 不適の場合, 幅と高さの値が負数になるので, 0でクリッピングする\n","    intersection_width_height = \\\n","     (intersection_right_down - intersection_left_top).clamp(min=0)\n","\n","    # [矩形数, 矩形数, 2] -\u003e [矩形数, 矩形数, 1]\n","    intersection_areas = intersection_width_height.prod(dim=2)\n","\n","    # それぞれの矩形の面積を計算 [矩形数, 1]\n","    # boxes1's (max_x - min_x) * (max_y - min_y)\n","    areas1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n","    # boxes2's (max_x - min_x) * (max_y - min_y)\n","    areas2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n","\n","    # 和集合の面積を計算\n","    # [矩形数, 1, 1(面積)] +\n","    # [矩形数, 1(矩形数)] =\n","    # [矩形数, 矩形数, 1]\n","    union_areas = areas1.unsqueeze(dim=1) + areas2 - intersection_areas\n","\n","    ious = intersection_areas / union_areas\n","\n","    return ious, union_areas"]},{"cell_type":"markdown","metadata":{"id":"BNx_L43kMNUd"},"source":["## xmin,ymin,xmax,ymax \u003c-\u003e cx,cy,w,h 変換"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7gQBtbzMYzF"},"outputs":[],"source":["'''\n","boxes: 矩形集合, [矩形数(任意), 4 (xmin,ymin,xmax,ymax)]\n","'''\n","def convert_to_xywh(boxes: torch.Tensor):\n","    wh = boxes[..., 2:] - boxes[..., :2]\n","    xy = boxes[..., :2] + wh / 2\n","    boxes = torch.cat((xy, wh), dim=-1)\n","    return boxes\n","\n","'''\n","boxes: 矩形集合, [矩形数(任意), 4 (cx,cy,w,h)]\n","'''\n","def convert_to_xyxy(boxes: torch.Tensor):\n","    xymin = boxes[..., :2] - boxes[..., 2:] / 2\n","    xymax = boxes[..., 2:] + xymin\n","    boxes = torch.cat((xymin, xymax), dim=-1)\n","    return boxes"]},{"cell_type":"markdown","metadata":{"id":"Quq9Fwq6OqP6"},"source":["## データセットの分割関数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwHgN4tmOtUh"},"outputs":[],"source":["'''\n","データセットを分割するための2つの排反なインデックス集合を生成する関数\n","dataset: 分割対象のデータセット\n","ratio: 1つ目のセットに含めるデータ量の割合\n","random_seed: シード値\n","'''\n","def generate_subset(dataset: Dataset,\n","                    ratio: float,\n","                    random_seed: int=0):\n","    # サブセットの大きさ\n","    size = int(len(dataset) * ratio)\n","\n","    indices = list(range(len(dataset)))\n","\n","    # シャッフル\n","    random.seed(random_seed)\n","    random.shuffle(indices)\n","\n","    indices1, indices2 = indices[:size], indices[size:]\n","\n","    return indices1, indices2"]},{"cell_type":"markdown","metadata":{"id":"qldrtNSRTAmD"},"source":["## データセット COCO2014\n","+ 約33万枚の画像データ\n","+ インターネットから集められた日常画像\n","+ 物体検出用のアノテーションは80クラスの外接矩形\n","+ その他, 作者によるセグメンテーション, 姿勢推定など\n","+ 2014年時点 学習用8万枚, 検証用4万枚 テスト用4万枚\n"]},{"cell_type":"markdown","metadata":{"id":"H666y_dsUS4p"},"source":["## val2014.zipをダウンロード\n","+ 検証用のみ使う\n","+ 1万枚\n","+ URL: http://images.cocodataset.org/zips/val2014.zip"]},{"cell_type":"markdown","metadata":{"id":"Nimd_vxOc8k7"},"source":["### GoogleDriveに保存先を変更"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"id":"y88cG5YGS6x7"},"outputs":[],"source":["!wget http://images.cocodataset.org/zips/val2014.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Rx2hYOjU8vH"},"outputs":[],"source":["# !ls -al"]},{"cell_type":"markdown","metadata":{"id":"qiIeqBygWZBP"},"source":["+ unarアプリ(zip解凍)をインストール\n","+ WindowsでZip化したファイルも文字化けしない"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8leR4A63WOJQ"},"outputs":[],"source":["# !apt install unar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h1pHIY9LATyV"},"outputs":[],"source":["import shutil\n","shutil.rmtree('val2014')"]},{"cell_type":"markdown","metadata":{"id":"371G4_BC7YbQ"},"source":["謎の解凍バグがあるので使わないことにした. '24/3/31"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"id":"pkpYnwNw5Qmk"},"outputs":[],"source":["!unzip val2014.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"id":"LJ0sSnCw7ig0"},"outputs":[],"source":["# お試し\n","from IPython.display import Image, display\n","ws_dir: str = os.getcwd()\n","file_path: str = os.path.join(ws_dir, f\"val2014/COCO_val2014_000000441814.jpg\")\n","display(Image(file_path))"]},{"cell_type":"markdown","metadata":{"id":"ok2lNzW-dGrD"},"source":["## アノテーションデータのLoad\n","+ 人と車の2種類\n","+ instances_val2014_small.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KRgLYw5WLXD"},"outputs":[],"source":["# !unar val2014.zip"]},{"cell_type":"markdown","metadata":{"id":"Q_7_XGSCdiyf"},"source":["## 独自のCocoDetection データセットクラス\n","+ 画像変形時に正解矩形も追従させる柔軟性を持つ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xy94tgqzdrnK"},"outputs":[],"source":["class CocoDetection(torchvision.datasets.CocoDetection):\n","    '''\n","    物体検出用COCOデータセット読み込みクラス\n","    img_directory: 画像ファイルが保存されているディレクトリパス\n","    anno_file: アノテーションファイル\n","    transforms: データ拡張と整形を行うクラスインスタンス\n","    '''\n","    def __init__(self,\n","                 img_directory: str,\n","                 anno_file: str,\n","                 transform: Callable=None,\n","                 ):\n","        super().__init__(img_directory, anno_file)\n","\n","        self.transform = transform\n","\n","        # カテゴリIDに欠番があるため、それを埋めてクラスIDを割り当て\n","        self.classes = []\n","        # 元々のクラスIDと新しく割り当てたクラスIDを相互に変換するためのマッピング\n","        self.coco_to_pred = {}\n","        self.pred_to_coco = {}\n","        for i, category_id in enumerate(sorted(self.coco.cats.keys())):\n","            self.classes.append(self.coco.cats[category_id]['name'])\n","            # category_id: 欠番のある1から始まるCocoラベル\n","            # i: 0から始まる再割り当てラベル\n","            self.coco_to_pred[category_id] = i\n","            self.pred_to_coco[i] = category_id\n","\n","    '''\n","    データ取得関数\n","    idx: サンプルを示すインデックス\n","    '''\n","    def __getitem__(self,\n","                    idx: int,\n","                    ):\n","        # print('__getitem__のオーバーロード')\n","\n","        # imgはPIL.Image\n","        pil_img, target = super().__getitem__(idx)\n","\n","        # 親クラスのコンストラクタでself.idsに\n","        # 画像IDが格納されているのでそれを取得\n","        img_id = self.ids[idx]\n","\n","        # 物体の集合を1つの矩形でアノテーションしている物を除外\n","        # アノテーション(jsonに`iscrowd`がないもの or iscrowd==1のもの)\n","        target = [obj for obj in target\n","                  if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n","\n","        # 学習用に該当画像に映る物体のクラスIDと矩形を取得\n","        # クラスIDはコンストラクタで新規に割り当てたIDに変換\n","        classes = torch.tensor(\n","            [self.coco_to_pred[obj['category_id']] for obj in target],\n","            dtype=torch.int64)\n","        boxes = torch.tensor(\n","            [obj['bbox'] for obj in target],\n","            dtype=torch.float32)\n","\n","        # 矩形が0個の時, boxes.shape == [0]となってしまうため,\n","        # 第一軸に4を追加して軸数と第二軸の次元を合わせる\n","        if boxes.shape[0] == 0:\n","            boxes = torch.zeros((0, 4)) # torch.size((0,4))\n","\n","        width, height = pil_img.size # PIL.Image\n","        # min_x, min_y, width, height =\u003e min_x, min_y, max_x, max_y\n","        boxes[:, 2:] += boxes[:, :2] # (正解矩形数, 4)\n","\n","        # 矩形が画像領域内に収まるように値をクリッピング\n","        boxes[:, ::2] = boxes[:, ::2].clamp(min=0, max=width)\n","        boxes[:, 1::2] = boxes[:, 1::2].clamp(min=0, max=height)\n","\n","        # ↑ ここまでで、1枚の画像から正解矩形データを複数枚抜き出したことになる\n","\n","        # 学習のための正解データを用意\n","        # クラスIDや矩形など渡すものが多岐にわたるので、辞書で用意\n","        target = {\n","            'image_id': torch.tensor(img_id, dtype=torch.int64),\n","            'classes': classes,\n","            'boxes': boxes,\n","            'size': torch.tensor((width, height), dtype=torch.int64),\n","            'orig_size': torch.tensor((width, height), dtype=torch.int64),\n","            'orig_img': torch.tensor(np.asarray(pil_img))\n","        }\n","\n","        # print('type(target) at __getitem__ of CocoDetection', type(target))\n","\n","        # データ拡張と整形\n","        if self.transform is not None:\n","            pil_img, target = self.transform(pil_img, target)\n","\n","        return pil_img, target\n","\n","    '''\n","    モデルで予測されたクラスIDからCOCOのクラスIDに変換する関数\n","    label: 予測されたクラスID\n","    '''\n","    def to_coco_label(self, label: int):\n","        return self.pred_to_coco[label]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"id":"mbuduZ2Ol3_M"},"outputs":[],"source":["# お試し\n","boxes = torch.zeros((0, 4)) # (4,)ではない\n","print(boxes.size())\n","print(boxes.shape)\n","print(boxes.numpy().shape)\n","print(boxes.numpy())\n","import numpy as np\n","vec1 = np.zeros((4,))\n","print(vec1.shape)\n","print(vec1)\n","vec2 = np.zeros((0,4))\n","print(vec2.shape)\n","print(vec2)"]},{"cell_type":"markdown","metadata":{"id":"Ab92xsST3_Cr"},"source":["## データ拡張"]},{"cell_type":"markdown","metadata":{"id":"iHQuE6mt4BdP"},"source":["### 水平反転"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDngiecmZcZ_"},"outputs":[],"source":["class RandomHorizontalFlip:\n","    def __init__(self, prob: float=0.5):\n","        self.prob = prob\n","\n","    def __call__(self,\n","                 img: Image,\n","                 target: dict,\n","                 ):\n","        pil_img = img\n","        if random.random() \u003c self.prob:\n","            pil_img = TF.hflip(pil_img)\n","\n","        # 正解矩形をx軸方向に反転\n","        # 制約式: max_x - min_x = width\n","        # min_x -\u003e width - max_x\n","        # max_x -\u003e width - min_x\n","        width = pil_img.size[0]\n","        # print(\"type(target)\", type(target))\n","        # print('target', target)\n","        target['boxes'][:, [0, 2]] = width - target['boxes'][:, [2, 0]]\n","\n","        return pil_img, target"]},{"cell_type":"markdown","metadata":{"id":"oPDdyBVBbJDl"},"source":["お試し"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGLarVVubIbO"},"outputs":[],"source":["# my_coco_ds = COCODetection(\n","\n","# )"]},{"cell_type":"markdown","metadata":{"id":"UcYx4c5v5ix7"},"source":["### クロップ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4KKR5d0A3vf"},"outputs":[],"source":["class RandomSizeCrop:\n","    '''\n","    scale: 切り抜き前に対する切り抜き後の画像面積の下限と上限\n","    ratio: 切り抜き後の画像のアスペクト比の下限と上限\n","    '''\n","    def __init__(self,\n","                 scale: Sequence[float],\n","                 ratio: Sequence[float],\n","                 ):\n","        self.scale = scale\n","        self.ratio = ratio\n","\n","    '''\n","    無作為に画像を切り抜く\n","    '''\n","    def __call__(self,\n","                 pil_img: Image,\n","                 target: dict,\n","                 ):\n","        width, height = pil_img.size\n","\n","        # 切り抜く領域の左上の座標と幅および高さを取得\n","        # 切り抜く領域はscaleとratioの下限と上限に従う\n","        top, left, cropped_height, cropped_width = \\\n","            T.RandomResizedCrop.get_params(pil_img,\n","                                          self.scale,\n","                                          self.ratio)\n","        # 左上の座標と幅および高さで指定した領域を切り抜き\n","        pil_img = TF.crop(pil_img, top, left, cropped_height, cropped_width)\n","\n","        # 原点がx=left,y=topになるように矩形座標を平行移動\n","        # (min_x, min_y, max_x, max_y)\n","        target['boxes'][:, ::2] -= left # min_x, max_x\n","        target['boxes'][:, 1::2] -= top # min_y, max_y\n","\n","        # 矩形の座標が切り抜き後に領域外に出る場合は座標をクリップする\n","        target['boxes'][:, ::2] = \\\n","            target['boxes'][:, ::2].clamp(min=0) # min_x, max_x\n","        target['boxes'][:, 1::2] = \\\n","            target['boxes'][:, 1::2].clamp(min=0) # min_y, max_y\n","        target['boxes'][:, ::2] = \\\n","            target['boxes'][:, ::2].clamp(max=cropped_width) # min_x, max_x\n","        target['boxes'][:, 1::2] = \\\n","            target['boxes'][:, 1::2].clamp(max=cropped_height) # min_y, max_y\n","\n","        # 幅と高さが0より大きくなる矩形のみを保持(max_x \u003e min_x \u0026 max_y \u003e min_y)\n","        # (矩形数=画像内のインスタンス数, 1)\n","        keep = (target['boxes'][:, 2] \u003e target['boxes'][:, 0]) \u0026 \\\n","               (target['boxes'][:, 3] \u003e target['boxes'][:, 1]) # マスク\n","        target['classes'] = target['classes'][keep]\n","        target['boxes'] = target['boxes'][keep]\n","\n","        # 切り抜き後の画像の大きさを保持\n","        target['size'] = torch.tensor([cropped_width, cropped_height], dtype=torch.int64)\n","\n","        return pil_img, target"]},{"cell_type":"markdown","metadata":{"id":"43PkbfI5d0KS"},"source":["お試し"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EhUt2rHad1Xb"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3Ht-wUQYAz1f"},"source":["### リサイズ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"av1ZHxF05PRX"},"outputs":[],"source":["class RandomResize:\n","    '''\n","    無作為に画像をアスペクト比を保持してリサイズするクラス\n","    min_sizes: 短辺の長さの候補、この中から無作為に長さを抽出\n","    max_size :  長辺の長さの最大値\n","    '''\n","    def __init__(self, min_sizes: Sequence[int], max_size: int):\n","        self.min_sizes = min_sizes\n","        self.max_size = max_size\n","\n","\n","    def _get_target_size(self,\n","                         min_size: int,\n","                         max_size: int,\n","                         target: int,\n","                         ):\n","        # アスペクト比を保持して短辺をtargetに合わせる\n","        max_size = int(max_size * target / min_size)\n","        min_size = target\n","\n","        # 長辺がmax_sideを超えている場合\n","        # アスペクト比を保持して長辺をmax_sizeに合わせる\n","        # このとき, 短辺は, (self.max_size / max_size)倍する\n","        # つまり, min_sideはtargetから更に短くなる\n","        if max_size \u003e self.max_size:\n","            min_size = int(min_size * self.max_size / max_size)\n","            max_size = self.max_size\n","\n","        return min_size, max_size\n","\n","    def __call__(self,\n","                 pil_img: Image,\n","                 target: dict,\n","                 ):\n","        # 短編の長さを候補の中から無作為に抽出\n","        min_size = random.choice(self.min_sizes)\n","\n","        width, height = pil_img.size\n","\n","        # リサイズ後の大きさを取得\n","        # 幅と高さのどちらが短編であるか場合分け\n","        if width \u003c height:\n","            resized_width, resized_height = self._get_target_size(\n","                min_size=width, max_size=height, target=min_size)\n","        else:\n","            resized_height, resized_width = self._get_target_size(\n","                min_size=height, max_size=width, target=min_size)\n","\n","        # 指定した大きさに画像をリサイズ\n","        pil_img = TF.resize(pil_img, (resized_height, resized_width))\n","\n","        # 正解矩形をリサイズ前後のスケールに合わせて変更\n","        ratio = resized_width / resized_height\n","        target['boxes'] *= ratio\n","\n","        # リサイズ後の画像の大きさを保存\n","        target['size'] = torch.tensor(\n","            [resized_width, resized_height], dtype=torch.int64\n","        )\n","\n","        return pil_img, target"]},{"cell_type":"markdown","metadata":{"id":"EAPU2dJlloXe"},"source":["お試し"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3NpF2kelpg2"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9qVJKt-86fqs"},"source":["## 画像整形"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPi7f-c66Yzf"},"outputs":[],"source":["class ToTensor:\n","    # PIL -\u003e Tensor\n","    def __call__(self,\n","                 img: Image,\n","                 target: dict,\n","                 ):\n","        img = TF.to_tensor(img)\n","        return img, target"]},{"cell_type":"markdown","metadata":{"id":"v3WSC3nHlqLa"},"source":["お試し"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BbuiEuZclsQu"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"xAm1tTxW61mK"},"source":["## 画像を標準化"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQWToypy6zsx"},"outputs":[],"source":["class Normalize:\n","    # mean(r,g,b)\n","    # std (r,g,b)\n","    def __init__(self,\n","                 mean: Sequence[float],\n","                 std: Sequence[float],\n","                 ):\n","        self.mean = mean\n","        self.std = std\n","\n","    def __call__(self,\n","                 img: torch.Tensor,\n","                 target: dict,\n","                 ):\n","        img = TF.normalize(img,\n","                          mean=self.mean,\n","                          std=self.std,\n","                          )\n","        return img, target"]},{"cell_type":"markdown","metadata":{"id":"8Tbiov7Elvv-"},"source":["お試し"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApFLRGRolxOI"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"4j772w5Ol4kX"},"source":["## ランダム選択"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNJ7cHP185lU"},"outputs":[],"source":["class RandomSelect:\n","    # transform1: データ拡張1\n","    # transform2: データ拡張2\n","    # prob: データ拡張1が適用される確率\n","    def __init__(self,\n","                 transform1: Callable,\n","                 transform2: Callable,\n","                 prob: float=0.5,\n","                 ):\n","        self.transform1 = transform1\n","        self.transform2 = transform2\n","        self.prob = prob\n","\n","    def __call__(self,\n","                 img: Image,\n","                 target: dict,\n","                 ):\n","        if random.random() \u003c self.prob:\n","            return self.transform1(img, target)\n","\n","        return self.transform2(img, target)"]},{"cell_type":"markdown","metadata":{"id":"dTm5SUZI8ewS"},"source":["## Compose"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWyIi_T47h2H"},"outputs":[],"source":["class Compose:\n","    def __init__(self,\n","                 transforms: Sequence[Callable],\n","                 ):\n","        self.transforms = transforms\n","\n","    def __call__(self,\n","                 img: Image,\n","                 target: dict,\n","                 ):\n","        for transform in self.transforms:\n","            img, target = transform(img, target) # オリジナル アノテーション矩形も変形が必要\n","\n","        return img, target"]},{"cell_type":"markdown","metadata":{"id":"G035B9oP96l3"},"source":["# RetinaNet アーキテクチャ"]},{"cell_type":"markdown","metadata":{"id":"-UKQ_QLyFWuy"},"source":["## Pretrained ResNet Backborn by ImageNet"]},{"cell_type":"markdown","metadata":{"id":"NONifUdWF7KO"},"source":["### FrozenResidualBlock"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnnNwLpc95WA"},"outputs":[],"source":["class FrozenResidualBlock(nn.Module):\n","    def __init__(self,\n","                 in_channels: int,\n","                 out_channels: int,\n","                 stride: int = 1,\n","                 ):\n","        super().__init__()\n","\n","        # 残差接続\n","        # conv -\u003e batchnorm -\u003e activation -\u003e conv -\u003e batchnorm\n","        self.conv1 = nn.Conv2d(in_channels,\n","                               out_channels,\n","                               kernel_size=3,\n","                               stride=stride,\n","                               padding=1,\n","                               bias=False,\n","                               )\n","        self.bn1 = FrozenBatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels,\n","                               out_channels,\n","                               kernel_size=3,\n","                               padding=1,\n","                               bias=False,\n","                               )\n","        self.bn2 = FrozenBatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        # strideが1より大きいときにスキップ接続と残差接続から得られる\n","        # 特徴量マップの高さと幅をあわせるために、別途畳み込みを用意\n","        self.downsample = None\n","        if stride \u003e 1:\n","            self.downsample = nn.Sequential(\n","                nn.Conv2d(in_channels,\n","                          out_channels,\n","                          kernel_size=1,\n","                          stride=stride,\n","                          bias=False,\n","                          ),\n","                FrozenBatchNorm2d(out_channels)\n","            )\n","\n","    def forward(self,\n","                x: torch.Tensor,\n","                ):\n","        # x : (B,C,H,W)\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            x = self.downsample(x)\n","\n","        # 残差接続とスキップコネクションの合流\n","        out += x\n","\n","        out = self.relu(out)\n","\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"ajj5Tq6IJoF9"},"source":["### FrozenResNet18"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFkKUyFhJmF0"},"outputs":[],"source":["class FrozenResNet18(nn.Module):\n","    def __init__(self,\n","                 ):\n","        super().__init__()\n","\n","        # Entrance layer\n","        self.conv1 = nn.Conv2d(in_channels=3,\n","                               out_channels=64,\n","                               kernel_size=7,\n","                               stride=2,\n","                               padding=3,\n","                               bias=False,\n","                               )\n","        self.bn1 = FrozenBatchNorm2d(num_features=64)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.max_pool = nn.MaxPool2d(kernel_size=3,\n","                                     stride=2,\n","                                     padding=1,\n","                                     )\n","\n","        # 1層目\n","        self.layer1 = nn.Sequential(\n","            FrozenResidualBlock(in_channels=64, out_channels=64),\n","            FrozenResidualBlock(in_channels=64, out_channels=64),\n","        )\n","\n","        # 2層目\n","        self.layer2 = nn.Sequential(\n","            FrozenResidualBlock(in_channels=64, out_channels=128, stride=2),\n","            FrozenResidualBlock(in_channels=128, out_channels=128),\n","        )\n","\n","        # 3層目\n","        self.layer3 = nn.Sequential(\n","            FrozenResidualBlock(in_channels=128, out_channels=256, stride=2),\n","            FrozenResidualBlock(in_channels=256, out_channels=256),\n","        )\n","\n","        # 4層目\n","        self.layer4 = nn.Sequential(\n","            FrozenResidualBlock(in_channels=256, out_channels=512, stride=2),\n","            FrozenResidualBlock(in_channels=512, out_channels=512),\n","        )\n","\n","    def forward(self,\n","                x: torch.Tensor,\n","                ):\n","        # x : (B,C,H,W)\n","\n","        # Entrance layer\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.max_pool(x)\n","\n","        x = self.layer1(x)\n","        c3 = self.layer2(x)\n","        c4 = self.layer3(c3)\n","        c5 = self.layer4(c4)\n","\n","        return c3, c4, c5"]},{"cell_type":"markdown","metadata":{"id":"IqPbL58JfdMd"},"source":["## 特徴量ピラミッドネットワーク"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFZQvnC7N6T4"},"outputs":[],"source":["class FeaturePyramidNetwork(nn.Module):\n","    # num_features: 出力特徴量のチャネル数\n","\n","    def __init__(self,\n","                 num_features: int=256,\n","                 ):\n","        super().__init__()\n","\n","        # 特徴ピラミッドネットワークから出力される階層レベル\n","        # バックボーンネットワークの最終層の特徴マップを第５階層とし、\n","        # 縮小方向に第6, 7層の2つの特徴マップを生成\n","        # 拡大方向に第3, 4層の2つの特徴マップを生成\n","        self.levels = (3,4,5,6,7)\n","\n","        # 縮小方向の特徴抽出\n","        self.p6 = nn.Conv2d(in_channels=512,\n","                            out_channels=num_features,\n","                            kernel_size=3,\n","                            stride=2, # より大域的な特徴量を抽出\n","                            padding=1,\n","                            )\n","\n","        self.p7_relu = nn.ReLU(inplace=True)\n","        self.p7 = nn.Conv2d(in_channels=num_features,\n","                            out_channels=num_features,\n","                            kernel_size=3,\n","                            stride=2,\n","                            padding=1,\n","                            )\n","\n","        # 拡大方向の特徴抽出\n","        self.p5_1 = nn.Conv2d(in_channels=512,\n","                              out_channels=num_features,\n","                              stride=1,\n","                              kernel_size=1,\n","                              padding=0,\n","                              ) # (K,S,P)=(1,1,0) -\u003e same\n","        self.p5_2 = nn.Conv2d(in_channels=num_features,\n","                              out_channels=num_features,\n","                              kernel_size=3,\n","                              stride=1,\n","                              padding=1,\n","                              ) # (K,S,P)=(3,1,1) -\u003e half\n","        self.p4_1 = nn.Conv2d(in_channels=256,\n","                              out_channels=num_features,\n","                              kernel_size=1,\n","                              stride=1,\n","                              padding=0,\n","                              ) # (K,S,P)=(1,1,0) -\u003e same\n","        self.p4_2 = nn.Conv2d(in_channels=256,\n","                              out_channels=num_features,\n","                              kernel_size=3,\n","                              stride=1,\n","                              padding=1,\n","                              ) # (K,S,P)=(3,1,1) -\u003e half\n","        self.p3_1 = nn.Conv2d(in_channels=128,\n","                              out_channels=num_features,\n","                              kernel_size=1,\n","                              stride=1,\n","                              padding=0,\n","                              ) # (K,S,P)=(1,1,0) -\u003e same\n","        self.p3_2 = nn.Conv2d(in_channels=num_features,\n","                              out_channels=num_features,\n","                              kernel_size=3,\n","                              stride=1,\n","                              padding=1,\n","                              ) # (K,S,P)=(3,1,1) -\u003e half\n","\n","    def forward(self,\n","                c3: torch.Tensor,\n","                c4: torch.Tensor,\n","                c5: torch.Tensor,\n","                ):\n","        # 縮小方向\n","        p6 = self.p6(c5)\n","        p7 = self.p7_relu(p6)\n","        p7 = self.p7(p7) # (B, C=num_features, H=c5_H/2, W=c5_W/2)\n","\n","        # 拡大方向\n","        p5 = self.p5_1(c5)\n","        p5_up = F.interpolate(p5, scale_factor=2) # 特徴マップの縦横サイズを2倍にする\n","        p5 = self.p5_2(p5)\n","\n","        p4 = self.p4_1(c4)\n","        p4_up = F.interpolate(p4, scale_factor=2)\n","        p4 = self.p4_2(p4)\n","\n","        p3 = self.p3_1(c3)\n","        p3_up = F.interpolate(p3, scale_factor=2)\n","        p3 = self.p3_2(p3_up)\n","\n","        return p3, p4, p5, p6, p7"]},{"cell_type":"markdown","metadata":{"id":"1P11ivgymIyA"},"source":["## 検出ヘッドネットワーク"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDZWGnrKmH4U"},"outputs":[],"source":["class DetectionHead(nn.Module):\n","    # 検出ヘッド(分類や矩形の回帰に使用する)\n","    # num_channels_per_anchor: 1アンカーに必要な出力チャネル数(特徴量数)\n","    # num_anchors: アンカー数\n","    # num_features: 入力及び中間特徴量のチャネル数\n","    def __init__(self,\n","                 num_channels_per_anchor: int,\n","                 num_anchors: int=9,\n","                 num_features: int=256,\n","                 ):\n","        super().__init__()\n","\n","        self.num_anchors = num_anchors\n","\n","        # 特徴ピラミッドネットワークの特徴マップを分類や回帰専用の\n","        # 特徴マップに変換するための畳み込みブロック\n","        self.conv_blocks = nn.ModuleList([\n","            nn.Sequential(nn.Conv2d(in_channels=num_features,\n","                                    out_channels=num_features,\n","                                    kernel_size=3,\n","                                    stride=1,\n","                                    padding=1),\n","                          nn.ReLU(inplace=True)) for _ in range(4)])\n","\n","        # 検出ヘッドの出力チャネル数を設定する\n","        # 分類ヘッドに使用する場合, アンカーボックス数 x 物体クラス数\n","        # 矩形ヘッドに使用する場合, アンカーボックス数 x 4 (cx,cy,w,h)\n","        self.out_conv = nn.Conv2d(\n","            in_channels=num_features,\n","            out_channels=num_anchors * num_channels_per_anchor,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1,\n","        ) # (K,S,P) = (3,1,1) -\u003e half\n","\n","    def forward(self,\n","                x: torch.Tensor, # (B,C,H,W)\n","                ):\n","        # convを4回実行\n","        for i in range(4):\n","            x = self.conv_blocks[i](x)\n","        x = self.out_conv(x)\n","\n","        bs, c, h, w = x.shape\n","\n","        # 後処理に備えて予測結果を並べ替える\n","        # permute関数(参照でない)\n","        # (B,C,H,W) -\u003e (B,H,W,C)\n","        x = x.permute(0, 2, 3, 1)\n","        # 第一軸に全画素の予測結果を並べる\n","        # (B,H,W,C) -\u003e (B, H*W*anchors, classes+4[x,y,w,h])\n","        x = x.reshape(bs, w * h * self.num_anchors, -1)\n","\n","        '''\n","        | clazz + 4 [cx,cy,w,h] |\n","        | ... |\n","        | ... |\n","        ↓ H*W*ancors\n","        '''\n","\n","\n","        return x\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ydQKhFpo4uVq"},"source":["## アンカーボックス生成器"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYrGJ3WQ4zzi"},"outputs":[],"source":["class AnchorGenerator:\n","    '''\n","    検出の基準となるアンカーボックスを生成するクラス\n","    levels: 入力特徴マップの階層\n","    '''\n","    def __init__(self,\n","                 levels: int,\n","                 ):\n","        # 用意するアンカーボックスのアスペクト比(ハイパーパラメータ)\n","        ratios: torch.tensor = torch.tensor([0.5, 1.0, 2.0])\n","\n","        # 用意するアンカーボックスの基準となる大きさに\n","        # 対するスケール(ハイパーパラメータ)\n","        scales = torch.tensor([2 ** 0, 2 ** (1/3), 2 ** (2/3)])\n","\n","        # 1つのアスペクト比に対して全スケールのアンカーボックスを\n","        # 用意するので、アンカーボックスの数は\n","        # アスペクト比の数 * スケール数\n","        self.num_anchors = ratios.shape[0] * scales.shape[0]\n","\n","        # 各階層の特徴マップでの1画素の移動量が入力画像での何画素の\n","        # 移動になるかを表す数値\n","        # 2**N のスケールで縮小するので, 1画素の移動量入力画像では2**N倍される\n","        self.strides = [2 ** level for level in levels]\n","\n","        self.anchors = []\n","        for level in levels:\n","            # 現階層における基準となる正方形のアンカーボックスの1辺の長さ\n","            # 深い階層のアンカーボックスには大きい物体の\n","            # 検出を担当させるため, 基準の長さを長く設定\n","            base_length = 2 ** (level + 2)\n","            # 0: 2**2 = 4\n","            # 1: 2**3 = 8\n","            # 2: 2**4 = 16\n","            # 3: 2**5 = 32\n","            # 4: 2**6 = 64\n","            # 5: 2**7 = 128\n","            # 6: 2**8 = 256\n","            # 7: 2**9 = 512\n","\n","            # アンカーボックスの1辺の長さをスケール\n","            scaled_lengths = base_length * scales\n","            # アンカーボックスが正方形の場合の面積を計算\n","            anchor_areas = scaled_lengths ** 2 # (3,)\n","\n","            # アスペクト比(ratio=height/ratio)に応じて辺の長さを変更\n","            # width * height = width * (width * ratio) = area\n","            # width = (area / ratio) ** 0.5\n","            # unsqueezeとブロードキャストにより\n","            # アスペクト比 * スケール数の数のアンカーボックスの幅と高さを生成\n","            # (3,) * (3,1) = (3, 3)\n","            # e.g\n","            # a = [1, 3.5, 6]\n","            # b = [[0.5], [1], [2]]\n","            # a*b = [\n","            #  [ 0.5, 1.75, 3],\n","            #  [ 1, 3.5, 6],\n","            #  [ 2, 7, 12]\n","            # ]\n","            anchor_widths = (anchor_areas / ratios.unsqueeze(1)) ** 0.5\n","            anchor_heights = anchor_widths * ratios.unsqueeze(1) # (3,3)\n","\n","            # (3,3) -\u003e (9,)\n","            anchor_widths = anchor_widths.flatten()\n","            anchor_heights = anchor_heights.flatten()\n","\n","            # アンカーボックスの中心を原点(0,0)としたときの\n","            # x_min, y_min, x_max, y_maxのオフセット\n","            anchor_x_mins = - 0.5 * anchor_widths\n","            anchor_y_mins = - 0.5 * anchor_heights\n","            anchor_x_maxs = 0.5 * anchor_widths\n","            anchor_y_maxs = 0.5 * anchor_heights\n","\n","            level_anchors = torch.stack(\n","                (anchor_x_mins, anchor_y_mins, anchor_x_maxs, anchor_y_maxs),\n","                dim=1) # (4, 9)\n","\n","            self.anchors.append(level_anchors)\n","\n","    # 関数内で勾配計算をさせないことを明示\n","    @torch.no_grad()\n","    def generate(self,\n","                 feature_sizes: Sequence[torch.Size],\n","                 ):\n","        '''\n","        アンカーボックス生成関数 :  b 入力画像座標上\n","        feature_sizes: 入力される複数の特徴マップぞれぞれの大きさ\n","        '''\n","        anchors = []\n","        # stride: (L, 1)\n","        # level_anchors: (L,9,4)\n","        # feature_size: (L, H_p[i]],W_p[i]) i= 1,...,L\n","        for stride, level_anchors, feature_size in zip(self.strides, self.anchors, feature_sizes):\n","            # 現階層の特徴マップの大きさ\n","            height, width = feature_size\n","\n","            # 入力画像の画素の移動量を表すstridesを使って\n","            # 特徴マップの画素の位置 -\u003e 入力画僧の画素の位置に変換\n","            # (画像の中心位置を計算するために0.5を加算)\n","            # x_at_in = 2^l * (x + 0.5), y_at_in = 2^l * (y + 0.5)\n","            xs = (torch.arange(width) + 0.5) * stride # 入力画像上\n","            ys = (torch.arange(height) + 0.5) * stride # 入力画像上\n","\n","            # 入力画像座標上のグリッド(x,y)\n","            grid_x, grid_y = torch.meshgrid(xs, ys, indexing='xy')\n","\n","            grid_x = grid_x.flatten() # (W_p[i]*H_p[i]),)\n","            grid_y = grid_y.flatten() # (H_p[i]*W_p[i],)\n","\n","            # 各画像の中心位置にアンカーボックスの\n","            # x_min,y_min,x_max,y_maxのオフセットを加算\n","            anchor_x_mins = (grid_x.unsqueeze(1) + level_anchors[:, 0]).flatten()\n","            anchor_y_mins = (grid_y.unsqueeze(1) + level_anchors[:, 1]).flatten()\n","            anchor_x_maxs = (grid_x.unsqueeze(1) + level_anchors[:, 2]).flatten()\n","            anchor_y_maxs = (grid_y.unsqueeze(1) + level_anchors[:, 3]).flatten()\n","\n","            # 第1軸を追加してx_min, y_min, x_max, y_maxを連結\n","            level_anchors = torch.stack(\n","                (anchor_x_mins, anchor_y_mins, anchor_x_maxs, anchor_y_maxs),\n","                dim=1\n","            ) # (H_p[i]*W_p[i]*9, 4)\n","            anchors.append(level_anchors) # list[(H_p[i]*W_p[i]*9, 4), ...] = (L,(H_p[i]*W_p[i]*9, 4)\n","\n","        # 全階層のアンカーボックスを連結\n","        anchors = torch.cat(anchors, dim=0) # (sum(H_p[i]*W_p[i]*9), 4)\n","\n","        return anchors\n"]},{"cell_type":"markdown","metadata":{"id":"pjeKO2sTRHq_"},"source":["## RetinaNet全体"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BF26BY3tPY6e"},"outputs":[],"source":["class RetinaNet(nn.Module):\n","    '''\n","    RetinaNetモデル(backborn=ResNet18)\n","    num_classes: 物体クラス数\n","    '''\n","    def __init__(self,\n","                 num_classes: int,\n","                 ):\n","        super().__init__()\n","\n","        # バックボーン(基礎特徴抽出)\n","        self.backbone = FrozenResNet18()\n","\n","        # 特徴量の精錬\n","        self.fpn = FeaturePyramidNetwork()\n","\n","        # アンカーボックス生成器\n","        self.anchor_generator = AnchorGenerator(self.fpn.levels)\n","\n","        # 分類及び矩形ヘッド\n","        #　検出ヘッドはすべての特徴マップで共有\n","        self.class_head = DetectionHead(\n","            num_channels_per_anchor=num_classes,\n","            num_anchors=self.anchor_generator.num_anchors,\n","        )\n","\n","        # num_channels_per_anchor=4は\n","        # (x_diff, y_diff, w_diff, h_diff)を推論するため\n","        self.box_head = DetectionHead(\n","            num_channels_per_anchor=4,\n","            num_anchors=self.anchor_generator.num_anchors,\n","        )\n","\n","        self._reset_parameters()\n","\n","    '''\n","    パラメータの初期化\n","    '''\n","    def _reset_parameters(self):\n","        for module in self.modules():\n","            if isinstance(module, nn.Conv2d):\n","                nn.init.kaiming_normal_(module.weight,\n","                                        mode='fan_out',\n","                                        nonlinearity='relu')\n","        # 分類ヘッドの出力にシグモイドを適用して各クラスの確率を出力\n","        # 学習開始時の確率が0.01になるようにパラメータを初期化\n","        prior = torch.tensor(0.01)\n","        nn.init.zeros_(self.class_head.out_conv.weight)\n","        nn.init.constant_(self.class_head.out_conv.bias,\n","                          -((1.0 - prior) / prior).log())\n","\n","        # 学習開始時のアンカーボックスの中心位置の移動が0,\n","        # 大きさが1倍となるように矩形ヘッドを初期化\n","        nn.init.zeros_(self.box_head.out_conv.weight)\n","        nn.init.zeros_(self.box_head.out_conv.bias)\n","\n","\n","    '''\n","    準伝搬\n","    x: 入力画像 (B, C, H, W)\n","    '''\n","    def forward(self, x: torch.Tensor):\n","        cs = self.backbone(x)\n","        ps = self.fpn(*cs) # p3,p4,p5,p6,p7\n","\n","        # 各特徴量マップ(p3,p4,p5,p6,p7)の各画素に対して\n","        # 9個のアンカーボックスが割り当てられている.\n","\n","        # p3,p4,p5,p6,p7に対して\n","        # 分類ヘッドと矩形ヘッドを適用(パラメータ共有)\n","        class_head_out_list = list(map(self.class_head, ps))\n","        box_head_out_list = list(map(self.box_head, ps))\n","\n","        '''各特徴量マップに対するヘッドの結果を連結'''\n","        # [(B, H_p3*W_p3*anchors, classes), ..., (B, H_p7*W_p7*anchors, classes)]\n","        preds_class = torch.cat(class_head_out_list, dim=1) # (B, sum(H_p[i]*W_p[i]*anchor)), classes)\n","        preds_box = torch.cat(box_head_out_list, dim=1) # (B, sum(H_p[i]*W_p[i]*anchor)), 4)\n","\n","        '''アンカーボックスを生成'''\n","        feature_sizes = [p.shape[2:] for p in ps] #[(H_p3,W_p3),(H_p4,W_p4),(H_p5,W_p5),(H_p6,W_p6),(H_p7,W_p7)]\n","        anchors = self.anchor_generator.generate(feature_sizes) # (levels=5, H_p[i]*W_p[i]*9, 4)\n","        anchors = anchors.to(x.device) # cpu -\u003e cuda\n","\n","        return preds_class, preds_box, anchors\n","\n","    '''\n","    モデルパラメータが保持されているデバイスを返す関数\n","    '''\n","    def get_device(self):\n","        return self.backbone.conv1.weight.device"]},{"cell_type":"markdown","metadata":{"id":"WGcl6L7LmLWH"},"source":["## 後処理\n","+ アンカーボックスと予測誤差の統合\n","+ 余分な検出矩形の除去"]},{"cell_type":"markdown","metadata":{"id":"A67rilFHmfdY"},"source":["アンカーボックスと予測誤差の統合"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgR1TdoumqFU"},"outputs":[],"source":["'''\n","preds_class : 検出矩形のクラス\n","              (B, sum(H_p[i]*W_p[i]*anchor))=アンカーボックスの数, classes)\n","preds_box : 検出矩形のアンカーボックスからの誤差\n","              (B, sum(H_p[i]*W_p[i]*anchor))=アンカーボックスの数, 4)\n","anchors : アンカーボックス\n","              (sum(H_p[i]*W_p[i]*9)=アンカーボックスの数, 4)\n","targets: ラベル\n","conf_threshold : 信頼度のしきい値\n","nms_threshold : NMSのIoUしきい値\n","'''\n","@torch.no_grad()\n","def post_process(preds_class: torch.Tensor,\n","                 preds_box: torch.Tensor,\n","                 anchors: torch.Tensor,\n","                 targets: dict,\n","                 conf_threshold: float = 0.05,\n","                 nms_threshold: float = 0.5,\n","                 ):\n","    '''矩形の整形'''\n","    batch_size = preds_class.shape[0]\n","\n","    anchors_xywh = convert_to_xywh(anchors)\n","\n","    # 中心座標の予測をスケール不変にするため\n","    # 予測値をアンカーボックスの大きさでスケールする\n","    # ネットワークの出力=誤差と見なす.\n","    # 中心 -\u003e (xr,yr) = (xa+xp*wa, ya+yp*ha)\n","    # 幅高 -\u003e (wr,hr) = (wa*exp(wp), ha*exp(hp))\n","    # -\u003e log(wr,hr) = (log(wp)+wp, log(hp)+hp) つまり誤差はlog空間の値\n","    preds_box[:, :, :2] = anchors_xywh[:, :2] + preds_box[:, :, :2] * anchors_xywh[:, :2] # (left,top)\n","    preds_box[:, :, 2:] = preds_box[:, :, 2:].exp() * anchors_xywh[:, 2:] # (w,h)\n","\n","    preds_box = convert_to_xyxy(preds_box)\n","\n","    '''矩形の除去'''\n","\n","    # 物体クラスの予測確率をシグモイド関数で計算\n","    # RetinaNetでは背景クラスは存在せず、\n","    # 背景を表す場合は、全ての物体クラスの予測確率が低くなるように実装されている\n","    preds_class = preds_class.sigmoid()\n","\n","    # 画像ごとの処理\n","    scores = []\n","    labels = []\n","    boxes = []\n","    for img_preds_class, img_preds_box, img_targets in zip(\n","        preds_class, preds_box, targets\n","    ):\n","        # 検出矩形が画像内に収まるように座標をクリップ\n","        img_preds_box[:, ::2] = img_preds_box[:, ::2].clamp(\n","            min=0, max=img_targets['size'][0]) # (xmin,ymin)\n","        img_preds_box[:, 1::2] = img_preds_box[:, 1::2].clamp(\n","            min=0, max=img_targets['size'][1]) # (xmax,ymax)\n","\n","        # 検出矩形は入力画像の大きさに合わせたものになっているので、\n","        # 元々の画像に合わせて検出矩形をスケールする\n","        img_preds_box *= img_targets['orig_size'][0] / img_targets['size'][0]\n","\n","        # 物体クラスのスコアとクラスIDを取得\n","        img_preds_score, img_preds_label = img_preds_class.max(dim=1)\n","\n","        # 信頼度がしきい値より高い検出矩形のみを残す\n","        keep = img_preds_score \u003e conf_threshold\n","        img_preds_score = img_preds_score[keep]\n","        img_preds_label = img_preds_label[keep]\n","        img_preds_box = img_preds_box[keep]\n","\n","        # クラス毎にNMSを適用\n","        keep_indices = batched_nms(img_preds_box,\n","                                   img_preds_score,\n","                                   img_preds_label,\n","                                   nms_threshold)\n","\n","        scores.append(img_preds_score[keep_indices])\n","        labels.append(img_preds_label[keep_indices])\n","        boxes.append(img_preds_box[keep_indices])\n","\n","    return scores, labels, boxes\n"]},{"cell_type":"markdown","metadata":{"id":"-tAdcVIFQej5"},"source":["## 損失関数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nm-XKGhLx70j"},"outputs":[],"source":["'''\n","preds_class : 検出矩形のクラス\n","(B, sum(H_p[i]*W_p[i]*anchor))=アンカーボックスの数, classes)\n","\n","preds_box : 検出矩形のアンカーボックスからの誤差\n","(B, sum(H_p[i]*W_p[i]*anchor))=アンカーボックスの数, 4)\n","\n","anchors : アンカーボックス\n","(sum(H_p[i]*W_p[i]*9)=アンカーボックスの数, 4)\n","\n","targets: ラベル\n","list[target(dict), ...]\n","\n","iou_lower_threshold : 検出矩形と正解矩形をマッチさせるか決める下限値\n","iou_higher_threshold : 検出矩形と正解矩形をマッチさせるか決める上限値\n","'''\n","def loss_func(preds_class: torch.Tensor,\n","              preds_box: torch.Tensor,\n","              anchors: torch.Tensor,\n","              targets: dict,\n","              iou_lower_threshold: float = 0.4,\n","              iou_higher_threshold: float = 0.5,\n","              ):\n","    anchors_xywh = convert_to_xywh(anchors)\n","\n","    # 画像ごとに目的関数を計算\n","    loss_class = preds_class.new_tensor(0)\n","    loss_box = preds_class.new_tensor(0)\n","    for img_preds_class, img_preds_box, img_targets in zip(\n","            preds_class, preds_box, targets):\n","\n","        # i) 現在の画像に対する正解矩形がないとき\n","        if img_targets['classes'].shape[0] == 0:\n","            # 全ての物体クラスの確率が0となるように\n","            # (背景クラスとして分類されるように)ラベルを作成\n","            targets_class = torch.zeros_like(img_preds_class)\n","            # https://pytorch.org/vision/stable/generated/torchvision.ops.sigmoid_focal_loss.html#torchvision.ops.sigmoid_focal_loss\n","            loss_class += sigmoid_focal_loss(\n","                img_preds_class, targets_class, reduction='sum')\n","\n","            continue\n","\n","        # 各画素のアンカーボックスと正解矩形のIoUを計算し,\n","        # 各アンカーボックスに対して最大のIoUを持つ正解矩形を抽出\n","        ious, _ = calc_iou(anchors, img_targets['boxes'])\n","        # ious: (アンカーボックス数, 正解矩形数)\n","        ious_max, ious_argmax = ious.max(dim=1) # (anchors, 1), (anchors, 1)\n","\n","        # 分類ラベルを-1で初期化\n","        # IoUが下限値と上限値にあるアンカーボックスは\n","        # ラベルを-1として損失を計算しないようにする\n","        targets_class = torch.full_like(img_preds_class, -1) # (anchors, classes)\n","\n","        # ii) IoUが下限値以下は, 背景(確率0)=[0,...,0]とする\n","        # print(\"\")\n","        # print('type(ious_max)', type(ious_max))\n","        # print('ious_max.size()', ious_max.size())\n","        ious_lower_masks = ious_max \u003c iou_lower_threshold\n","        # print('ious_lower_masks', ious_lower_masks)\n","        # print('ious_max \u003c iou_lower_threshold @ size', (ious_max \u003c iou_lower_threshold).size())\n","        targets_class[ious_lower_masks] = 0\n","        # print('targets_class', targets_class)\n","\n","        # iii) IoUが上限値以上は, 陽性のアンカーボックスとして分類回帰の対象にする\n","        positive_masks = ious_max \u003e iou_higher_threshold # (anchors, 1)\n","        num_positive_anchors = positive_masks.sum()\n","\n","        # 陽性のアンカーボックスについて、マッチした正解矩形が示す\n","        # 物体クラスの確率を1, それ以外を0にする. e.g. [0,0,1,...,0]\n","        targets_class[positive_masks] = 0 # クラス確率を初期化\n","        assigned_classes = img_targets['classes'][ious_argmax] # (正解矩形数, classes)\n","        targets_class[positive_masks,\n","                      assigned_classes[positive_masks]] = 1 # クラスラベルの列に1を立てる\n","\n","        # iv) IoUが下限値と上限値の間にある(つまり背景orクラス割り当てが不明瞭)な\n","        # アンカーボックスについては, 分類の損失計算を行わない\n","        targets_masks = targets_class != -1\n","        valid_losses = targets_masks * sigmoid_focal_loss(img_preds_class, targets_class)\n","        # ここでは, num_positive_anchors == 0のケースがあるので, 0割エラーを回避する.\n","        # この場合, valid_lossesは全て0の多次元配列なので, valid_losses.sum()もゼロ\n","        loss_class += valid_losses.sum() / num_positive_anchors.clamp(min=1)\n","\n","        # 陽性のアンカーボックスが一つも存在しない場合\n","        # 矩形の誤差計算はしない\n","        if num_positive_anchors == 0:\n","            continue\n","\n","        # 各アンカーボックスとマッチした正解矩形を抽出\n","        assigned_boxes = img_targets['boxes'][ious_argmax] # (正解矩形数, 4[xmin,ymin,xmax,ymax])\n","        assigned_boxes_xywh = convert_to_xywh(assigned_boxes)\n","\n","        ''' アンカーボックスとマッチした正解矩形との誤差計算 '''\n","        targets_box = torch.zeros_like(img_preds_box) # (anchors, 4)\n","\n","        # 中心位置の誤差はアンカーボックスの大きさでスケールする\n","        # (xy_target - xy_anchor) / wh_anchor\n","        targets_box[:, :2] = \\\n","         (assigned_boxes_xywh[:, :2] - anchors_xywh[:, :2]) / anchors_xywh[:, 2:]\n","\n","        # 矩形の幅と高さはアンカーボックスに対するスケールのLOG空間で予測\n","        targets_box[:, 2:] = (assigned_boxes_xywh[:,2:] / anchors_xywh[:, 2:]).log()\n","\n","        # Smooth L1 誤差 (L1誤差とL2誤差の組み合わせ)を矩形回帰に使用\n","        loss_box += F.smooth_l1_loss(img_preds_box[positive_masks],\n","                                     targets_box[positive_masks],\n","                                     beta=1/9)\n","    # lossをバッチサイズで割る\n","    batch_size = preds_class.shape[0]\n","    loss_class = loss_class / batch_size\n","    loss_box = loss_box / batch_size\n","\n","    return loss_class, loss_box\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mASse7QRrAie"},"source":["## ハイパーパラメータの設定"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hPanmewq3gg"},"outputs":[],"source":["class ConfigTrainEval:\n","    def __init__(self):\n","        self.img_directory = \"/content/val2014\"\n","        self.anno_file = \"/content/drive/MyDrive/ColabNotebooks/Book_DL_Image_Recognition/data/coco2014/instances_val2014_small.json\"\n","        self.save_file = \"/content/drive/MyDrive/ColabNotebooks/Book_DL_Image_Recognition/model/retinanet.pth\"\n","        self.val_ratio = 0.2\n","        self.num_epochs = 50\n","        self.lr_drop = 45\n","        self.val_interval = 5\n","        self.lr = 1e-5\n","        self.clip = 0.1 # 勾配クリップ上限\n","        self.moving_avg = 100 # 移動平均で計算する損失と正確度の値の数\n","        self.batch_size = 8\n","        self.num_workers = 2\n","        self.device = 'cuda'"]},{"cell_type":"markdown","metadata":{"id":"LacXDzMIxynz"},"source":["## お試し"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"id":"yPDomVV4x2ko"},"outputs":[],"source":["# お試し\n","\n","# データ拡張・整形クラスの設定\n","min_sizes = (480, 512, 544, 576, 608)\n","\n","train_transforms = Compose((\n","    RandomHorizontalFlip(),\n","    RandomSelect(\n","        RandomResize(min_sizes, max_size=1024),\n","        Compose((\n","            RandomSizeCrop(scale=(0.8, 1.0),\n","                            ratio=(0.75, 1.333)),\n","            RandomResize(min_sizes, max_size=1024),\n","        ))\n","    ),\n","    ToTensor(),\n","    # ImageNetの平均と標準偏差\n","    Normalize(mean=(0.485, 0.456, 0.406),\n","                std=(0.229, 0.224, 0.225)),\n","))\n","\n","\n","naive_trn_coco_ds = CocoDetection(\n","    img_directory=\"/content/val2014\",\n","    anno_file=\"/content/drive/MyDrive/ColabNotebooks/Book_DL_Image_Recognition/data/coco2014/instances_val2014_small.json\",\n","    # transform=train_transforms\n",")\n","\n","naive_record = naive_trn_coco_ds[0]\n","naive_target: dict = naive_record[1]\n","print(naive_target['orig_img'].size())\n","\n","transform_trn_coco_ds = CocoDetection(\n","    img_directory=\"/content/val2014\",\n","    anno_file=\"/content/drive/MyDrive/ColabNotebooks/Book_DL_Image_Recognition/data/coco2014/instances_val2014_small.json\",\n","    transform=train_transforms\n",")\n","\n","transform_record = transform_trn_coco_ds[0]\n","transform_target = transform_record[1]\n","print(transform_target['orig_img'].size())"]},{"cell_type":"markdown","metadata":{"id":"oKbUJsmX2RKG"},"source":["# 学習と評価"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUVbUIFU2P9T"},"outputs":[],"source":["def train_eval():\n","    config = ConfigTrainEval()\n","\n","    # データ拡張・整形クラスの設定\n","    min_sizes = (480, 512, 544, 576, 608)\n","\n","    train_transforms = Compose((\n","        RandomHorizontalFlip(),\n","        RandomSelect(\n","            RandomResize(min_sizes, max_size=1024),\n","            Compose((\n","                RandomSizeCrop(scale=(0.8, 1.0),\n","                               ratio=(0.75, 1.333)),\n","                RandomResize(min_sizes, max_size=1024),\n","            ))\n","        ),\n","        ToTensor(),\n","        # ImageNetの平均と標準偏差\n","        Normalize(mean=(0.485, 0.456, 0.406),\n","                    std=(0.229, 0.224, 0.225)),\n","    ))\n","\n","    test_transforms = Compose((\n","        # テストは短編最大で実行\n","        RandomResize((min_sizes[-1],), max_size=1024),\n","        ToTensor(),\n","        # ImageNetの平均と標準偏差\n","        Normalize(mean=(0.485, 0.456, 0.406),\n","                    std=(0.229, 0.224, 0.225)),\n","    ))\n","\n","    # データセット\n","    train_dataset = CocoDetection(\n","        img_directory=config.img_directory,\n","        anno_file=config.anno_file,\n","        transform=train_transforms,\n","    )\n","    val_dataset = CocoDetection(\n","        img_directory=config.img_directory,\n","        anno_file=config.anno_file,\n","        transform=test_transforms,\n","    )\n","\n","    # Subset samplerの生成\n","    val_set, train_set = generate_subset(train_dataset, config.val_ratio)\n","\n","    print(f\"学習セットのサンプル数: {len(train_set)}\")\n","    print(f\"検証セットのサンプル数: {len(val_set)}\")\n","\n","    # 学習時にランダムにサンプルするためのサンプラー\n","    train_sampler = SubsetRandomSampler(train_set)\n","\n","    # Dataloader\n","    train_loader = DataLoader(\n","        train_dataset, batch_size=config.batch_size,\n","        num_workers=config.num_workers, sampler=train_sampler,\n","        collate_fn=collate_func,\n","    )\n","    val_loader = DataLoader(\n","        val_dataset, batch_size=config.batch_size,\n","        num_workers=config.num_workers, sampler=val_set,\n","        collate_fn=collate_func,\n","    )\n","\n","    # RetinaNet\n","    model = RetinaNet(len(train_dataset.classes))\n","    # ResNet18をImageNetの学習済みモデルで初期化\n","    # 最後の全結合層が無いなどのモデルの改変を許容するためにstrict=False\n","    model.backbone.load_state_dict(torch.hub.load_state_dict_from_url(\n","        'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n","    ), strict=False)\n","\n","    model.to(config.device)\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=config.lr)\n","\n","    scheduler = optim.lr_scheduler.MultiStepLR(\n","        optimizer, milestones=[config.lr_drop], gamma=0.1\n","    )\n","\n","    # Epochループ\n","    for epoch in range(config.num_epochs):\n","        model.train()\n","\n","        with tqdm(train_loader) as pbar:\n","            pbar.set_description(f\"[エポック {epoch + 1}]\")\n","\n","            # 移動平均計算用\n","            losses_class = deque()\n","            losses_box = deque()\n","            losses = deque()\n","\n","            # 学習\n","            for imgs, targets in pbar:\n","                imgs = imgs.to(model.get_device())\n","                targets = [{\n","                    k: v.to(model.get_device())\n","                    for k, v in target.items()\n","                } for target in targets]\n","\n","                optimizer.zero_grad()\n","\n","                preds_class, preds_box, anchors = model(imgs)\n","\n","                loss_class, loss_box = loss_func(\n","                    preds_class, preds_box, anchors, targets\n","                )\n","                loss = loss_class + loss_box\n","\n","                loss.backward()\n","\n","                # 勾配全体のL2ノルムが上限を超えるとき上限値でクリッピング\n","                torch.nn.utils.clip_grad_norm_(\n","                    model.parameters(), config.clip\n","                )\n","\n","                optimizer.step()\n","\n","                losses_class.append(loss_class.item())\n","                losses_box.append(loss_box.item())\n","                losses.append(loss.item())\n","                if len(losses) \u003e config.moving_avg:\n","                    losses_class.popleft()\n","                    losses_box.popleft()\n","                    losses.popleft()\n","                pbar.set_postfix({\n","                    'loss': torch.Tensor(losses).mean().item(),\n","                    'loss_class': torch.Tensor(losses_class).mean().item(),\n","                    'loss_box': torch.Tensor(losses_box).mean().item()\n","                })\n","\n","            # スケジューラでエポック数をカウント\n","            scheduler.step()\n","\n","            # パラメータを保存\n","            torch.save(model.state_dict(), config.save_file)\n","\n","            # 検証\n","            if (epoch + 1) % config.val_interval == 0:\n","                evaluate(val_loader, model, loss_func)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ig5dGfmgEyq5"},"source":["## サンプルからミニバッチを生成するcollate関数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kv9GVoI8E18l"},"outputs":[],"source":["'''\n","batch: CocoDetectionからサンプルした複数枚の画像とラベルをまとめたもの\n","'''\n","def collate_func(batch: Sequence[Tuple[Union[torch.Tensor, dict]]]):\n","    # ミニバッチの中の画像で最大の高さと幅を取得\n","    max_height = 0\n","    max_width = 0\n","    for img, _ in batch:\n","        height, width = img.shape[1:]\n","        max_height = max(max_height, height)\n","        max_width = max(max_width, width)\n","\n","    # バックボーンネットワークで特徴マップの解像度を下げるときに\n","    # 切捨てが起きないように入力の幅と高さを32の倍数にしておく\n","    # もし32の倍数でない場合, バックボーンネットワークの特徴マップと\n","    # 特徴ピラミッドネットワークのUpScalingでできた特徴マップの\n","    # 大きさに不整合が生じ, 加算できなくなる\n","    height = (max_height + 31) // 32 * 32\n","    width = (max_width + 31) // 32 * 32\n","\n","    # 画像を一つのテンソルにまとめる\n","    # ラベルはリストに集約\n","    imgs = batch[0][0].new_zeros((len(batch), 3, height, width))\n","    targets = []\n","    for i, (img, target) in enumerate(batch):\n","        height, width = img.shape[1:]\n","        imgs[i, :, :height, :width] = img # パディング領域は0埋め\n","\n","        targets.append(target)\n","\n","    return imgs, targets"]},{"cell_type":"markdown","metadata":{"id":"SbRum9PPGfzE"},"source":["## 評価関数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1yM8dK0cGd41"},"outputs":[],"source":["def evaluate(data_loader: DataLoader,\n","             model: nn.Module,\n","             loss_func: Callable,\n","             conf_threshold: float = 0.05,\n","             nms_threshold: float = 0.5):\n","    model.eval()\n","\n","    losses_class = []\n","    losses_box = []\n","    losses = []\n","    preds = []\n","    img_ids = []\n","    for imgs, targets in tqdm(data_loader, desc='[Validation]'):\n","        with torch.no_grad():\n","            imgs = imgs.to(model.get_device())\n","            targets = [{\n","                k: v.to(model.get_device())\n","                for k, v in target.items()\n","            } for target in targets]\n","\n","            preds_class, preds_box, anchors = model(imgs)\n","\n","            loss_class, loss_box = loss_func(\n","                preds_class, preds_box, anchors, targets\n","            )\n","\n","            loss = loss_class + loss_box\n","\n","            losses_class.append(loss_class)\n","            losses_box.append(loss_box)\n","            losses.append(loss)\n","\n","        # 後処理により最終的な検出矩形を取得\n","        scores, labels, boxes = post_process(\n","            preds_class, preds_box, anchors, targets,\n","            conf_threshold=conf_threshold,\n","            nms_threshold=nms_threshold,\n","        )\n","\n","        for img_scores, img_labels, img_boxes, img_targets in zip(\n","            scores, labels, boxes, targets\n","        ):\n","            img_ids.append(img_targets['image_id'].item())\n","\n","            # 評価のためにCocoの元々の矩形表現である\n","            # xmin, ymin, width, heightに変換\n","            img_boxes[:, 2:] -= img_boxes[:, :2]\n","\n","            for score, label, box in zip(\n","                img_scores, img_labels, img_boxes\n","            ):\n","                # COCO評価用のデータの保存\n","                preds.append({\n","                    'image_id': img_targets['image_id'].item(),\n","                    'category_id': data_loader.dataset.to_coco_label(label.item()),\n","                    'score': score.item(),\n","                    'bbox': box.to('cpu').numpy().tolist()\n","                })\n","\n","    # 平均値\n","    loss_class = torch.stack(losses_class).mean().item()\n","    loss_box = torch.stack(losses_box).mean().item()\n","    loss = torch.stack(losses).mean().item()\n","    print(f\"Validation loss = {loss: .3f}, \"\n","          f\"class loss = {loss_class: .3f}, \"\n","          f\"box loss = {loss_box: .3f} \")\n","\n","    if len(preds) == 0:\n","        print('Nothing detected, skip evaluation')\n","        return\n","\n","    # COCOevalクラスを使って評価するには検出結果を\n","    # jsonファイルに出力する必要があるため, jsonファイルに一時保存\n","    with open('tmp.json', 'w') as f:\n","        json.dump(preds, f)\n","\n","    # 一時保存sたい結果をCOCOクラスを使って読み込み\n","    coco_results = data_loader.dataset.coco.loadRes('tmp.json')\n","\n","    # COCOevalクラスを使って評価\n","    coco_eval = COCOeval(\n","        data_loader.dataset.coco,\n","        coco_results,\n","        'bbox'\n","    )\n","    coco_eval.params.imgIds = img_ids\n","    coco_eval.evaluate()\n","    coco_eval.accumulate()\n","    coco_eval.summarize()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZfEzEQGzLrEj"},"source":["## 学習と評価"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"id":"3xFyeZa5LdZ-"},"outputs":[],"source":["train_eval()"]},{"cell_type":"markdown","metadata":{"id":"_LLLXV02L0eY"},"source":["# デモ"]},{"cell_type":"markdown","metadata":{"id":"_DFBM8EjL58q"},"source":["## デモにおけるハイパーパラメータの設定"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZE6-YP_bLzuN"},"outputs":[],"source":["class ConfigDemo:\n","    def __init__(self):\n","        self.img_directory = \"/content/drive/MyDrive/ColabNotebooks/Book_DL_Image_Recognition/data/object_detection\"\n","        self.load_file = \"/content/drive/MyDrive/ColabNotebooks/Book_DL_Image_Recognition/model/retinanet.pth\"\n","        self.classes = ['person', 'car']\n","        self.device = 'cuda'\n","        self.conf_threshold = 0.5\n","        self.nms_threshold = 0.5"]},{"cell_type":"markdown","metadata":{"id":"ZtXuQkAXMwju"},"source":["## デモ用の関数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgvY3rogMmkJ"},"outputs":[],"source":["from pathlib import Path\n","from torchvision.utils import draw_bounding_boxes\n","\n","def demo():\n","    config = ConfigDemo()\n","\n","    transforms = Compose((\n","        RandomResize((608,), max_size=1024),\n","        ToTensor(),\n","        Normalize(mean=(0.485, 0.456, 0.406),\n","                    std=(0.229, 0.224, 0.225)),\n","\n","    ))\n","\n","    # 学習済みモデルパラメータを読み込み\n","    model = RetinaNet(len(config.classes))\n","    model.load_state_dict(torch.load(config.load_file))\n","    model.to(config.device)\n","    model.eval()\n","\n","    for img_path in Path(config.img_directory).iterdir():\n","        img_orig = Image.open(img_path)\n","        width, height = img_orig.size\n","\n","        # データを整形を適用するために\n","        # ダミーデータをラベルの作成\n","        target = {\n","            'classes': torch.zeros((0,), dtype=torch.int64),\n","            'boxes': torch.zeros((0, 4), dtype=torch.float32),\n","            'size': torch.tensor((width, height), dtype=torch.int64),\n","            'orig_size': torch.tensor((width, height), dtype=torch.int64),\n","        }\n","\n","        # データ整形\n","        img, target = transforms(img_orig, target)\n","        imgs, targets = collate_func([(img, target)])\n","\n","        with torch.no_grad():\n","            imgs = imgs.to(model.get_device())\n","            targets = [{ k: v.to(model.get_device())\n","                            for k, v in target.items()\n","                        } for target in targets ]\n","\n","            preds_class, preds_box, anchors = model(imgs)\n","\n","            # 後処理\n","            scores, labels, boxes = post_process(\n","                preds_class, preds_box, anchors, targets,\n","                conf_threshold = config.conf_threshold,\n","                nms_threshold = config.nms_threshold\n","            )\n","\n","            # 描画用の画像を用意\n","            img = torch.tensor(np.asarray(img_orig))\n","            img = img.permute(2, 0, 1)\n","\n","            # クラスIDをクラス名に変換\n","            labels = [config.classes[label] for label in labels[0]]\n","\n","            # 矩形を描画\n","            img = draw_bounding_boxes(\n","                img, boxes[0], labels, colors='red',\n","                font='LiberationSans-Regular.ttf',\n","                font_size=42, width=4,\n","            )\n","            img = img.permute(1,2,0)\n","            img = img.to('cpu').numpy()\n","            img = Image.fromarray(img)\n","            display(img)\n"]},{"cell_type":"markdown","metadata":{"id":"S6x0rHmrUmQm"},"source":["## デモの実行"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"id":"wsSveLigUktH"},"outputs":[],"source":["\n","demo()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOL4jbWUwl4iL0wZ5BW6Y68","gpuType":"A100","machine_shape":"hm","mount_file_id":"1WHV8JijedPSwJvl26FypXdsesMYHCELd","name":"ch05_od_retinanet 2024-04-18T12:01:00Z .ipynb","provenance":[{"file_id":"1gWDHT_kDGRl18vaAsO1ctLs2D004vhQV","timestamp":1713441685916}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}